{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, snowball\nfrom nltk.stem.porter import *\nfrom nltk import word_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim import corpora, models\nfrom gensim.models import word2vec\nimport seaborn as sns\nfrom pprint import pprint\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n\nnltk.download('wordnet')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-24T08:21:37.120483Z","iopub.execute_input":"2022-04-24T08:21:37.121499Z","iopub.status.idle":"2022-04-24T08:21:39.492456Z","shell.execute_reply.started":"2022-04-24T08:21:37.121365Z","shell.execute_reply":"2022-04-24T08:21:39.491544Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1.0 Import dataset and explore dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/covid19-vaccine-news-reddit-discussions/comments.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:39.494046Z","iopub.execute_input":"2022-04-24T08:21:39.494289Z","iopub.status.idle":"2022-04-24T08:21:40.091008Z","shell.execute_reply.started":"2022-04-24T08:21:39.494261Z","shell.execute_reply":"2022-04-24T08:21:40.090179Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.092271Z","iopub.execute_input":"2022-04-24T08:21:40.093177Z","iopub.status.idle":"2022-04-24T08:21:40.102035Z","shell.execute_reply.started":"2022-04-24T08:21:40.093134Z","shell.execute_reply":"2022-04-24T08:21:40.101034Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"np.sum(pd.isnull(df))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.104620Z","iopub.execute_input":"2022-04-24T08:21:40.105193Z","iopub.status.idle":"2022-04-24T08:21:40.162408Z","shell.execute_reply.started":"2022-04-24T08:21:40.105143Z","shell.execute_reply":"2022-04-24T08:21:40.161113Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The data has 14 columns with 34768 observations with no null values. The data contains the post that the comment has been left, and the content of the post. Some of the features can be dropped since those features are redundant (i.e. post_id, post_author,comment_parent_id). We'll leave post_id and drop the other features. Also, since the post date for both post and comments are not the key factor in this assignment, therefore will drop those columns.\n\nMoreover, since the data observes the comments in the reddit website, there are parent post data called post_title. We can group comments based on these posts and explore more about it. \n\nComment score and post score can be further explored.","metadata":{}},{"cell_type":"code","source":"# drop post_author,comment_parent_id\n\ndf = df.drop(['post_author','comment_parent_id', 'post_date','post_permalink', 'comment_author', 'comment_date'], axis = 1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.163581Z","iopub.execute_input":"2022-04-24T08:21:40.164179Z","iopub.status.idle":"2022-04-24T08:21:40.194997Z","shell.execute_reply.started":"2022-04-24T08:21:40.164129Z","shell.execute_reply":"2022-04-24T08:21:40.194158Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"grouped = df[['post_score','comment_id','comment_score','comment_body']].groupby(df['post_title'])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.196246Z","iopub.execute_input":"2022-04-24T08:21:40.196528Z","iopub.status.idle":"2022-04-24T08:21:40.207685Z","shell.execute_reply.started":"2022-04-24T08:21:40.196497Z","shell.execute_reply":"2022-04-24T08:21:40.206689Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"group_freq = grouped.size().sort_values()\ngroup_freq = group_freq.to_frame()\ngroup_freq.columns = ['comment_freq']\ngroup_freq.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.208686Z","iopub.execute_input":"2022-04-24T08:21:40.209661Z","iopub.status.idle":"2022-04-24T08:21:40.234987Z","shell.execute_reply.started":"2022-04-24T08:21:40.209608Z","shell.execute_reply":"2022-04-24T08:21:40.234364Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"group_score = grouped.mean().sort_values(by = 'comment_score')\ngroup_score.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.236439Z","iopub.execute_input":"2022-04-24T08:21:40.236920Z","iopub.status.idle":"2022-04-24T08:21:40.252885Z","shell.execute_reply.started":"2022-04-24T08:21:40.236878Z","shell.execute_reply":"2022-04-24T08:21:40.251924Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"numeric_df = group_score.merge(group_freq, how='inner', on='post_title')\nnumeric_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.254188Z","iopub.execute_input":"2022-04-24T08:21:40.254665Z","iopub.status.idle":"2022-04-24T08:21:40.275551Z","shell.execute_reply.started":"2022-04-24T08:21:40.254633Z","shell.execute_reply":"2022-04-24T08:21:40.274729Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape of group_score: {group_score.shape}\")\nprint(f\"Shape of group_freq: {group_freq.shape}\")\nprint(f\"Shape of merged dataframe: {numeric_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.278233Z","iopub.execute_input":"2022-04-24T08:21:40.278606Z","iopub.status.idle":"2022-04-24T08:21:40.283903Z","shell.execute_reply.started":"2022-04-24T08:21:40.278571Z","shell.execute_reply":"2022-04-24T08:21:40.283263Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"grouped = df[['post_score','comment_id','comment_score','comment_body']].groupby(df['post_id'])\n\ngroup_freq = grouped.size().sort_values()\ngroup_freq = group_freq.to_frame()\ngroup_freq.columns = ['comment_freq']\n\ngroup_score = grouped.mean().sort_values(by = 'comment_score')\n\nnumeric_df = group_score.merge(group_freq, how='inner', on='post_id')\n\nprint(f\"Shape of group_score: {group_score.shape}\")\nprint(f\"Shape of group_freq: {group_freq.shape}\")\nprint(f\"Shape of merged dataframe: {numeric_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.285012Z","iopub.execute_input":"2022-04-24T08:21:40.285413Z","iopub.status.idle":"2022-04-24T08:21:40.312356Z","shell.execute_reply.started":"2022-04-24T08:21:40.285370Z","shell.execute_reply":"2022-04-24T08:21:40.311417Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Since the post title is a bit lengthy to be a suitable index, we could use post_id as a substitution. Just to be safe, we need to check if one user has posted multiple posts (then the rows of df grouped by 'post_title' and 'post_id' will be different). However, since the shape of the two different dfs are exactly same, we use df grouped by 'post_id'.","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Use pre-trained embedding to find similar words","metadata":{}},{"cell_type":"code","source":"STOP_WORDS = nltk.corpus.stopwords.words()\n\ndef lemmatizer(text):\n    Lemmatizer = WordNetLemmatizer()\n    return Lemmatizer.lemmatize(text)\n\ndef clean_sentence(val):\n    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n    regex = re.compile('([^\\s\\w]|_)')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)\n            \n    sentence = \" \".join(sentence)\n    return sentence\n\ndef clean_dataframe(data):\n    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n    data = data.dropna(how=\"any\")\n    \n    data['comment_body'] = data['comment_body'].apply(clean_sentence)\n    \n    return data\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.313741Z","iopub.execute_input":"2022-04-24T08:21:40.313974Z","iopub.status.idle":"2022-04-24T08:21:40.344132Z","shell.execute_reply.started":"2022-04-24T08:21:40.313949Z","shell.execute_reply":"2022-04-24T08:21:40.343438Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"embedded_df = clean_dataframe(df)\nembedded_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:21:40.345705Z","iopub.execute_input":"2022-04-24T08:21:40.346093Z","iopub.status.idle":"2022-04-24T08:22:54.351574Z","shell.execute_reply.started":"2022-04-24T08:21:40.346063Z","shell.execute_reply":"2022-04-24T08:22:54.350953Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def build_corpus(data):\n    \"Creates a list of lists containing words from each sentence\"\n    corpus = []\n    for sentence in df['comment_body'].iteritems():\n        #print(sentence)\n        word_list = sentence[1]\n        word_list = nltk.word_tokenize(word_list)\n        lemmatized_output = ' '.join([lemmatizer(w) for w in word_list])\n        corpus.append(lemmatized_output.split(\" \"))\n            \n    return corpus\n\ncorpus = build_corpus(embedded_df)        \ncorpus[0:2]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:22:54.352764Z","iopub.execute_input":"2022-04-24T08:22:54.353134Z","iopub.status.idle":"2022-04-24T08:23:26.431190Z","shell.execute_reply.started":"2022-04-24T08:22:54.353105Z","shell.execute_reply":"2022-04-24T08:23:26.430245Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def index_2d(myList, v):\n    for i, x in enumerate(myList):\n        if v in x:\n            return i, x.index(v)\n\nprint(index_2d(corpus,'sad'))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:23:26.432652Z","iopub.execute_input":"2022-04-24T08:23:26.432968Z","iopub.status.idle":"2022-04-24T08:23:26.439867Z","shell.execute_reply.started":"2022-04-24T08:23:26.432928Z","shell.execute_reply":"2022-04-24T08:23:26.438992Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = word2vec.Word2Vec(corpus, window=15, min_count=50, workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:23:26.441415Z","iopub.execute_input":"2022-04-24T08:23:26.441907Z","iopub.status.idle":"2022-04-24T08:23:33.298132Z","shell.execute_reply.started":"2022-04-24T08:23:26.441863Z","shell.execute_reply":"2022-04-24T08:23:33.297501Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model.wv['vaccine']","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:23:33.298972Z","iopub.execute_input":"2022-04-24T08:23:33.299182Z","iopub.status.idle":"2022-04-24T08:23:33.306066Z","shell.execute_reply.started":"2022-04-24T08:23:33.299158Z","shell.execute_reply":"2022-04-24T08:23:33.305489Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('vaccine')[:5]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:23:33.306940Z","iopub.execute_input":"2022-04-24T08:23:33.307426Z","iopub.status.idle":"2022-04-24T08:23:33.324287Z","shell.execute_reply.started":"2022-04-24T08:23:33.307394Z","shell.execute_reply":"2022-04-24T08:23:33.323141Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('sad')[:5]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:23:33.326618Z","iopub.execute_input":"2022-04-24T08:23:33.327375Z","iopub.status.idle":"2022-04-24T08:23:33.342014Z","shell.execute_reply.started":"2022-04-24T08:23:33.327306Z","shell.execute_reply":"2022-04-24T08:23:33.341049Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Visualization using T-SNE","metadata":{}},{"cell_type":"markdown","source":"Since the Gensim 4.0.0 model doesn't support vocab function, we use .index_to_key instead","metadata":{}},{"cell_type":"code","source":"def tsne_plot1(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n    \n\n    for word in model.wv.index_to_key:\n        tokens.append(model.wv[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        plt.title(\"T-SNE Model for all Word2vec Vocabulary\",fontsize=15)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:23:33.346833Z","iopub.execute_input":"2022-04-24T08:23:33.351027Z","iopub.status.idle":"2022-04-24T08:23:33.369136Z","shell.execute_reply.started":"2022-04-24T08:23:33.350948Z","shell.execute_reply":"2022-04-24T08:23:33.368072Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tsne_plot1(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:23:33.376463Z","iopub.execute_input":"2022-04-24T08:23:33.379994Z","iopub.status.idle":"2022-04-24T08:25:16.495248Z","shell.execute_reply.started":"2022-04-24T08:23:33.379897Z","shell.execute_reply":"2022-04-24T08:25:16.494619Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"We have intentionally reduced the size of the cells and the annotation since it was too messy to see any kind of pattern. However, although we have reduced the size, the t-sne plot is still chaotic, showing no special trend. Now we can do the t-sne plot for most frequent words.","metadata":{}},{"cell_type":"code","source":"labels = []\ntokens = []\n    \n\nfor word in model.wv.index_to_key:\n    tokens.append(model.wv[word])\n    labels.append(word)\n    \ntsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\nnew_values = tsne_model.fit_transform(tokens)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:25:16.496332Z","iopub.execute_input":"2022-04-24T08:25:16.497036Z","iopub.status.idle":"2022-04-24T08:25:54.994216Z","shell.execute_reply.started":"2022-04-24T08:25:16.497001Z","shell.execute_reply":"2022-04-24T08:25:54.993519Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"new_corpus = [single for sublist in corpus for single in sublist]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:25:54.998339Z","iopub.execute_input":"2022-04-24T08:25:55.000674Z","iopub.status.idle":"2022-04-24T08:25:55.066565Z","shell.execute_reply.started":"2022-04-24T08:25:55.000621Z","shell.execute_reply":"2022-04-24T08:25:55.065856Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"top_1000_freq = pd.value_counts(np.array(new_corpus))[:1000]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:25:55.070867Z","iopub.execute_input":"2022-04-24T08:25:55.072835Z","iopub.status.idle":"2022-04-24T08:25:57.495937Z","shell.execute_reply.started":"2022-04-24T08:25:55.072790Z","shell.execute_reply":"2022-04-24T08:25:57.495035Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def tsne_plot2(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n        \n    for word in top_1000_freq.index:\n        tokens.append(model.wv[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=[16, 16]) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i],s=200)\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        plt.title(\"T-SNE Model for top 1000 frequent Word2vec Vocabulary\",fontsize=15)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:25:57.497091Z","iopub.execute_input":"2022-04-24T08:25:57.497346Z","iopub.status.idle":"2022-04-24T08:25:57.534047Z","shell.execute_reply.started":"2022-04-24T08:25:57.497295Z","shell.execute_reply":"2022-04-24T08:25:57.533100Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"tsne_plot2(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:25:57.535474Z","iopub.execute_input":"2022-04-24T08:25:57.535795Z","iopub.status.idle":"2022-04-24T08:26:36.727991Z","shell.execute_reply.started":"2022-04-24T08:25:57.535752Z","shell.execute_reply":"2022-04-24T08:26:36.727043Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The T-SNE plot for top 1000 frequent vocab also shows the random pattern similar to the \"T-SNE Model for all Word2vec Vocabulary\". Since looking at this individually doesn't show any special observations, we can try comparing the \"T-SNE Model for all Word2vec Vocabulary\" and \"T-SNE Model for top 1000 frequent Word2vec Vocabulary\".","metadata":{}},{"cell_type":"code","source":"def compare_tnse1_tnse2(model):\n\n    fig = plt.figure(figsize = [30,15])\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2)\n    \n    # tnse_plot1\n    labels_tnse_plot1 = []\n    tokens_tnse_plot1 = []\n    \n    for word in model.wv.index_to_key:\n        tokens_tnse_plot1.append(model.wv[word])\n        labels_tnse_plot1.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot1)\n\n    x_tnse_plot1 = []\n    y_tnse_plot1 = []\n    for value in new_values:\n        x_tnse_plot1.append(value[0])\n        y_tnse_plot1.append(value[1])\n        \n    for i in range(len(x_tnse_plot1)):\n        ax1.scatter(x_tnse_plot1[i],y_tnse_plot1[i])\n        ax1.annotate(labels_tnse_plot1[i],\n                     xy=(x_tnse_plot1[i], y_tnse_plot1[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax1.set_title(\"T-SNE Model for all Word2vec Vocabulary\",fontsize=9)\n\n        \n    # tnse_plot2\n    labels_tnse_plot2 = []\n    tokens_tnse_plot2 = []\n        \n    for word in top_1000_freq.index:\n        tokens_tnse_plot2.append(model.wv[word])\n        labels_tnse_plot2.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot2)\n\n    x_tnse_plot2 = []\n    y_tnse_plot2 = []\n    for value in new_values:\n        x_tnse_plot2.append(value[0])\n        y_tnse_plot2.append(value[1])\n        \n    for i in range(len(x_tnse_plot2)):\n        ax2.scatter(x_tnse_plot2[i],y_tnse_plot2[i])\n        ax2.annotate(labels_tnse_plot2[i],\n                     xy=(x_tnse_plot2[i], y_tnse_plot2[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax2.set_title(\"T-SNE Model for top 1000 frequent Word2vec Vocabulary\",fontsize=9)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:26:36.729482Z","iopub.execute_input":"2022-04-24T08:26:36.730105Z","iopub.status.idle":"2022-04-24T08:26:36.745478Z","shell.execute_reply.started":"2022-04-24T08:26:36.730071Z","shell.execute_reply":"2022-04-24T08:26:36.744711Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"compare_tnse1_tnse2(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:26:36.749301Z","iopub.execute_input":"2022-04-24T08:26:36.750063Z","iopub.status.idle":"2022-04-24T08:29:01.127032Z","shell.execute_reply.started":"2022-04-24T08:26:36.750012Z","shell.execute_reply":"2022-04-24T08:29:01.126135Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"The comparison between \"T-SNE Model for all Word2Vec Vocabulary\" and \"T-SNE Model for top 1000 frequent Word2vec Vocabulary\" seems similar, just the observations shown in the right part of the \"T-SNE Model for all Word2Vec Vocabulary\" plot has been eliminated. But the interesting part is that since we have eliminated some words which haven't appear that much, the range of the observation has decreased; the range of y axis has decreased from -50 ~ 50 to -30 ~ 35 and x axis has decreased from -70 ~ 50 to -40 ~ 45. Maybe we can try observe more by selecting data with top 500 frequent vocabulary and top 100 frequent vocabulary.","metadata":{}},{"cell_type":"code","source":"top_100_freq = pd.value_counts(np.array(new_corpus))[:100]\ntop_500_freq = pd.value_counts(np.array(new_corpus))[:500]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:29:01.128338Z","iopub.execute_input":"2022-04-24T08:29:01.128574Z","iopub.status.idle":"2022-04-24T08:29:05.395538Z","shell.execute_reply.started":"2022-04-24T08:29:01.128548Z","shell.execute_reply":"2022-04-24T08:29:05.394456Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def tsne_plot3(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in top_100_freq.index:\n        tokens.append(model.wv[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=50, n_components=2,learning_rate = 500, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i], s= 100)\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 15,\n                     ha='right',\n                     va='bottom')\n        plt.legend()\n        plt.title(\"T-SNE Model for top 100 frequent Word2vec Vocabulary\",fontsize=9)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:29:05.396717Z","iopub.execute_input":"2022-04-24T08:29:05.396981Z","iopub.status.idle":"2022-04-24T08:29:05.407167Z","shell.execute_reply.started":"2022-04-24T08:29:05.396952Z","shell.execute_reply":"2022-04-24T08:29:05.406488Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"tsne_plot3(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:29:05.408496Z","iopub.execute_input":"2022-04-24T08:29:05.408780Z","iopub.status.idle":"2022-04-24T08:29:08.762569Z","shell.execute_reply.started":"2022-04-24T08:29:05.408741Z","shell.execute_reply":"2022-04-24T08:29:08.761615Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Just in case if there are some special patterns on the plot if we have limit the data with 100 most frequent vocabulary, the pattern seems random as it was for \"T-SNE Model for all Word2Vec Vocabulary\" and \"T-SNE Model for top 1000 frequent Word2vec Vocabulary\". We now try to compare all 4 models: \"T-SNE Model for all Word2Vec Vocabulary\", \"T-SNE Model for top 1000 frequent Word2vec Vocabulary\", \"T-SNE Model for top 500 frequent Word2vec Vocabulary\" and \"T-SNE Model for top 100 frequent Word2vec Vocabulary\"","metadata":{}},{"cell_type":"code","source":"def compare_tnse_models1(model):\n\n    fig = plt.figure(figsize = [30,30])\n    ax1 = fig.add_subplot(2, 2, 1)\n    ax2 = fig.add_subplot(2, 2, 2)\n    ax3 = fig.add_subplot(2, 2, 3)\n    ax4 = fig.add_subplot(2, 2, 4)\n    \n    # tnse_plot on word2vec model vocabs\n    labels_tnse_plot1 = []\n    tokens_tnse_plot1 = []\n    \n    for word in model.wv.index_to_key:\n        tokens_tnse_plot1.append(model.wv[word])\n        labels_tnse_plot1.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot1)\n\n    x_tnse_plot1 = []\n    y_tnse_plot1 = []\n    for value in new_values:\n        x_tnse_plot1.append(value[0])\n        y_tnse_plot1.append(value[1])\n        \n    for i in range(len(x_tnse_plot1)):\n        ax1.scatter(x_tnse_plot1[i],y_tnse_plot1[i], s = 100 )\n        ax1.annotate(labels_tnse_plot1[i],\n                     xy=(x_tnse_plot1[i], y_tnse_plot1[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax1.legend()\n        ax1.set_title(\"T-SNE Model for all Word2vec Vocabulary\",fontsize=9)\n    \n    # tnse_plot on top 1000 commonly seen words\n    labels_tnse_plot2 = []\n    tokens_tnse_plot2 = []\n        \n    for word in top_1000_freq.index:\n        tokens_tnse_plot2.append(model.wv[word])\n        labels_tnse_plot2.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot2)\n\n    x_tnse_plot2 = []\n    y_tnse_plot2 = []\n    for value in new_values:\n        x_tnse_plot2.append(value[0])\n        y_tnse_plot2.append(value[1])\n        \n    for i in range(len(x_tnse_plot2)):\n        ax2.scatter(x_tnse_plot2[i],y_tnse_plot2[i], s = 100 )\n        ax2.annotate(labels_tnse_plot2[i],\n                     xy=(x_tnse_plot2[i], y_tnse_plot2[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax2.legend()\n        ax2.set_title(\"T-SNE Model for top 1000 frequent Word2vec Vocabulary\",fontsize=9)\n\n    # tnse_plot on top 500 commonly seen words\n    labels_tnse_plot3 = []\n    tokens_tnse_plot3 = []\n        \n    for word in top_500_freq.index:\n        tokens_tnse_plot3.append(model.wv[word])\n        labels_tnse_plot3.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot3)\n\n    x_tnse_plot3 = []\n    y_tnse_plot3 = []\n    for value in new_values:\n        x_tnse_plot3.append(value[0])\n        y_tnse_plot3.append(value[1])\n        \n    for i in range(len(x_tnse_plot3)):\n        ax3.scatter(x_tnse_plot3[i],y_tnse_plot3[i], s = 100 )\n        ax3.annotate(labels_tnse_plot3[i],\n                     xy=(x_tnse_plot3[i], y_tnse_plot3[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax3.legend()\n        ax3.set_title(\"T-SNE Model for top 100 frequent Word2vec Vocabulary\",fontsize=9)\n        \n    # tnse_plot on top 100 commonly seen words\n    labels_tnse_plot4 = []\n    tokens_tnse_plot4 = []\n        \n    for word in top_100_freq.index:\n        tokens_tnse_plot4.append(model.wv[word])\n        labels_tnse_plot4.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot4)\n\n    x_tnse_plot4 = []\n    y_tnse_plot4 = []\n    for value in new_values:\n        x_tnse_plot4.append(value[0])\n        y_tnse_plot4.append(value[1])\n        \n    for i in range(len(x_tnse_plot4)):\n        ax4.scatter(x_tnse_plot4[i],y_tnse_plot4[i], s = 100 )\n        ax4.annotate(labels_tnse_plot4[i],\n                     xy=(x_tnse_plot4[i], y_tnse_plot4[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 10,\n                     ha='right',\n                     va='bottom')\n        ax4.legend()\n        ax4.set_title(\"T-SNE Model for top 100 frequent Word2vec Vocabulary\",fontsize=9)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:29:08.764048Z","iopub.execute_input":"2022-04-24T08:29:08.764283Z","iopub.status.idle":"2022-04-24T08:29:08.791700Z","shell.execute_reply.started":"2022-04-24T08:29:08.764257Z","shell.execute_reply":"2022-04-24T08:29:08.790600Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"compare_tnse_models1(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:29:08.793110Z","iopub.execute_input":"2022-04-24T08:29:08.793500Z","iopub.status.idle":"2022-04-24T08:32:09.276084Z","shell.execute_reply.started":"2022-04-24T08:29:08.793464Z","shell.execute_reply":"2022-04-24T08:32:09.275083Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"We can see that the range of the x axis and y axis in the plot has dramatically shrunken. we can conclude that mostly used word in comments are positioned in the center, compactly gathered together. Just to see how it's significant, we gave a weight based on the frequency for the next plot. ","metadata":{}},{"cell_type":"code","source":"def compare_tnse_models2(model):\n\n    fig = plt.figure(figsize = [30,30])\n    ax1 = fig.add_subplot(2, 2, 1)\n    ax2 = fig.add_subplot(2, 2, 2)\n    ax3 = fig.add_subplot(2, 2, 3)\n    ax4 = fig.add_subplot(2, 2, 4)\n    \n    # tnse_plot on word2vec model vocabs\n    labels_tnse_plot1 = []\n    tokens_tnse_plot1 = []\n    \n    for word in model.wv.index_to_key:\n        tokens_tnse_plot1.append(model.wv[word])\n        labels_tnse_plot1.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot1)\n\n    x_tnse_plot1 = []\n    y_tnse_plot1 = []\n    for value in new_values:\n        x_tnse_plot1.append(value[0])\n        y_tnse_plot1.append(value[1])\n        \n    for i in range(len(x_tnse_plot1)):\n        ax1.scatter(x_tnse_plot1[i],y_tnse_plot1[i], s = 100)\n        ax1.annotate(labels_tnse_plot1[i],\n                     xy=(x_tnse_plot1[i], y_tnse_plot1[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax1.legend()\n        ax1.set_title(\"T-SNE Model for all Word2vec Vocabulary\",fontsize=9)\n    \n    # tnse_plot on top 1000 commonly seen words\n    labels_tnse_plot2 = []\n    tokens_tnse_plot2 = []\n        \n    for word in top_1000_freq.index:\n        tokens_tnse_plot2.append(model.wv[word])\n        labels_tnse_plot2.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot2)\n\n    x_tnse_plot2 = []\n    y_tnse_plot2 = []\n    for value in new_values:\n        x_tnse_plot2.append(value[0])\n        y_tnse_plot2.append(value[1])\n        \n    for i in range(len(x_tnse_plot2)):\n        ax2.scatter(x_tnse_plot2[i],y_tnse_plot2[i], s = (100 * top_1000_freq[i])/50, alpha=0.5)\n        ax2.annotate(labels_tnse_plot2[i],\n                     xy=(x_tnse_plot2[i], y_tnse_plot2[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax2.legend()\n        ax2.set_title(\"T-SNE Model for top 1000 frequent Word2vec Vocabulary\",fontsize=9)\n\n    # tnse_plot on top 500 commonly seen words\n    labels_tnse_plot3 = []\n    tokens_tnse_plot3 = []\n        \n    for word in top_500_freq.index:\n        tokens_tnse_plot3.append(model.wv[word])\n        labels_tnse_plot3.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot3)\n\n    x_tnse_plot3 = []\n    y_tnse_plot3 = []\n    for value in new_values:\n        x_tnse_plot3.append(value[0])\n        y_tnse_plot3.append(value[1])\n        \n    for i in range(len(x_tnse_plot3)):\n        ax3.scatter(x_tnse_plot3[i],y_tnse_plot3[i], s = (100 * top_500_freq[i])/50, alpha=0.5)\n        ax3.annotate(labels_tnse_plot3[i],\n                     xy=(x_tnse_plot3[i], y_tnse_plot3[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 1,\n                     ha='right',\n                     va='bottom')\n        ax3.legend()\n        ax3.set_title(\"T-SNE Model for top 100 frequent Word2vec Vocabulary\",fontsize=9)\n        \n    # tnse_plot on top 100 commonly seen words\n    labels_tnse_plot4 = []\n    tokens_tnse_plot4 = []\n        \n    for word in top_100_freq.index:\n        tokens_tnse_plot4.append(model.wv[word])\n        labels_tnse_plot4.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=4300)\n    new_values = tsne_model.fit_transform(tokens_tnse_plot4)\n\n    x_tnse_plot4 = []\n    y_tnse_plot4 = []\n    for value in new_values:\n        x_tnse_plot4.append(value[0])\n        y_tnse_plot4.append(value[1])\n        \n    for i in range(len(x_tnse_plot4)):\n        ax4.scatter(x_tnse_plot4[i],y_tnse_plot4[i], s = (100 * top_100_freq[i])/50, alpha=0.5)\n        ax4.annotate(labels_tnse_plot4[i],\n                     xy=(x_tnse_plot4[i], y_tnse_plot4[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     size = 10,\n                     ha='right',\n                     va='bottom')\n        ax4.legend()\n        ax4.set_title(\"T-SNE Model for top 100 frequent Word2vec Vocabulary\",fontsize=9)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:32:09.277800Z","iopub.execute_input":"2022-04-24T08:32:09.278112Z","iopub.status.idle":"2022-04-24T08:32:09.311243Z","shell.execute_reply.started":"2022-04-24T08:32:09.278077Z","shell.execute_reply":"2022-04-24T08:32:09.310365Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"compare_tnse_models2(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:32:09.312958Z","iopub.execute_input":"2022-04-24T08:32:09.313340Z","iopub.status.idle":"2022-04-24T08:35:08.333969Z","shell.execute_reply.started":"2022-04-24T08:32:09.313279Z","shell.execute_reply":"2022-04-24T08:35:08.332640Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"As seen in this bubble graph, most of the bigger bubbles are positioned in the center which indicates mostly used words are positioned in the center. Looking at the frequently used words, they mostly have neutral nuances, which makes the plot showing compactly gathered bubbles. The extreme words, either positive or negative, tend to stay on the edge of the t-sne plot for all Word2Vec vocabulary. This is explainable for example, some people might have negative perspective on vaccine while the others have positive perspective on vaccine, thus the word vaccine is the frequently appeared word, while the words with extreme persepective tends to stay in edge. Just to make sure if our hypothesis is right. We'll now run tsne plot based on closest words. If the points in the plot are scatterd, then the hypothesis of mostly used vocabs are neutral words.","metadata":{}},{"cell_type":"code","source":"def display_closestwords_tsnescatterplot(model, word, size):\n    \n    arr = np.empty((0,size), dtype='f')\n    word_labels = [word]\n\n    close_words = model.wv.most_similar(word)\n\n    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model.wv[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    plt.scatter(x_coords, y_coords)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()+0.0005, x_coords.max()+0.0005)\n    plt.ylim(y_coords.min()+0.0005, y_coords.max()+0.0005)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:35:08.335499Z","iopub.execute_input":"2022-04-24T08:35:08.335764Z","iopub.status.idle":"2022-04-24T08:35:08.346410Z","shell.execute_reply.started":"2022-04-24T08:35:08.335735Z","shell.execute_reply":"2022-04-24T08:35:08.345721Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"display_closestwords_tsnescatterplot(model, 'vaccine', 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:35:08.347414Z","iopub.execute_input":"2022-04-24T08:35:08.347980Z","iopub.status.idle":"2022-04-24T08:35:08.933121Z","shell.execute_reply.started":"2022-04-24T08:35:08.347948Z","shell.execute_reply":"2022-04-24T08:35:08.932355Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"display_closestwords_tsnescatterplot(model, 'covid', 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:35:08.937222Z","iopub.execute_input":"2022-04-24T08:35:08.937766Z","iopub.status.idle":"2022-04-24T08:35:09.486426Z","shell.execute_reply.started":"2022-04-24T08:35:08.937724Z","shell.execute_reply":"2022-04-24T08:35:09.485700Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"display_closestwords_tsnescatterplot(model, 'distancing', 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:35:09.490542Z","iopub.execute_input":"2022-04-24T08:35:09.492576Z","iopub.status.idle":"2022-04-24T08:35:09.996545Z","shell.execute_reply.started":"2022-04-24T08:35:09.492508Z","shell.execute_reply":"2022-04-24T08:35:09.995841Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"display_closestwords_tsnescatterplot(model, 'government', 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:35:10.000682Z","iopub.execute_input":"2022-04-24T08:35:10.002675Z","iopub.status.idle":"2022-04-24T08:35:10.601931Z","shell.execute_reply.started":"2022-04-24T08:35:10.002627Z","shell.execute_reply":"2022-04-24T08:35:10.601221Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"The plot created by \"display_closestwords_tsnescatterplot\" function shows that the closest words are scattered all over the plot, thus reinforcing our hypothesis.","metadata":{}},{"cell_type":"markdown","source":"**Next part will be covered in different ipynb file since the computation time until this part is already too long to run all the part.**","metadata":{}}]}
