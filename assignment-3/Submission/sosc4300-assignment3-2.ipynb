{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install vaderSentiment\n\nimport numpy as np \nimport pandas as pd \nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, snowball\nfrom nltk.stem.porter import *\nfrom nltk import word_tokenize\nfrom nltk.tag import pos_tag\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim import corpora, models\nfrom gensim.models import word2vec\nimport seaborn as sns\nfrom pprint import pprint\nimport statsmodels.formula.api as smf\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nnltk.download('wordnet')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-24T01:30:15.618192Z","iopub.execute_input":"2022-04-24T01:30:15.618482Z","iopub.status.idle":"2022-04-24T01:30:23.737769Z","shell.execute_reply.started":"2022-04-24T01:30:15.618453Z","shell.execute_reply":"2022-04-24T01:30:23.736776Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/covid19-vaccine-news-reddit-discussions/comments.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:10.146188Z","iopub.execute_input":"2022-04-24T01:14:10.146543Z","iopub.status.idle":"2022-04-24T01:14:10.775066Z","shell.execute_reply.started":"2022-04-24T01:14:10.146501Z","shell.execute_reply":"2022-04-24T01:14:10.774044Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 2. Topic Modeling ","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.1 Topic Modeling in Verb","metadata":{}},{"cell_type":"code","source":"\"\"\" Testing the function\n# SnowballStemmer Example\n#the stemmer requires a language parameter\nsnow_stemmer = snowball.SnowballStemmer(language='english')\n  \n#list of tokenized words\nwords = ['cared','university','fairly','easily','singing',\n       'sings','sung','singer','sportingly']\n  \n#stem's of each word\nstem_words = []\nfor w in words:\n    x = snow_stemmer.stem(w)\n    stem_words.append(x)\n      \n#print stemming results\nfor e1,e2 in zip(words,stem_words):\n    print(e1+' ----> '+e2)\n\n# SnowballStemmer trial\nsnow_stemmer = snowball.SnowballStemmer(language='english')\nsnow_stemmer.stem('post')  \n\n# Creating function trial\nresult = []\ni = 0\nfor token in gensim.utils.simple_preprocess(df['comment_body'][0]):\n    print(f\"{i}'th term'\")\n    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n        print(token)\n        token = WordNetLemmatizer().lemmatize(token, pos='v')\n        print(token)\n        snow_stemmer = snowball.SnowballStemmer(language='english')\n        result.append(snow_stemmer.stem(token))\n    i +=1\nprint(result)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:10.776272Z","iopub.execute_input":"2022-04-24T01:14:10.776512Z","iopub.status.idle":"2022-04-24T01:14:10.785386Z","shell.execute_reply.started":"2022-04-24T01:14:10.776482Z","shell.execute_reply":"2022-04-24T01:14:10.784388Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def lemmatize_stemming_verb(text):\n    snow_stemmer = snowball.SnowballStemmer(language='english')\n    return snow_stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\ndef preprocess_verb(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming_verb(token))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:10.788780Z","iopub.execute_input":"2022-04-24T01:14:10.789390Z","iopub.status.idle":"2022-04-24T01:14:10.795915Z","shell.execute_reply.started":"2022-04-24T01:14:10.789340Z","shell.execute_reply":"2022-04-24T01:14:10.794929Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"verb_processed_docs = df['comment_body'].map(preprocess_verb)\nverb_processed_docs[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:10.797893Z","iopub.execute_input":"2022-04-24T01:14:10.798142Z","iopub.status.idle":"2022-04-24T01:14:32.256246Z","shell.execute_reply.started":"2022-04-24T01:14:10.798112Z","shell.execute_reply":"2022-04-24T01:14:32.255383Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(verb_processed_docs)\ncount = 0\n# check if it's well recorded\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:32.257405Z","iopub.execute_input":"2022-04-24T01:14:32.257668Z","iopub.status.idle":"2022-04-24T01:14:33.550466Z","shell.execute_reply.started":"2022-04-24T01:14:32.257615Z","shell.execute_reply":"2022-04-24T01:14:33.549523Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=5000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in verb_processed_docs]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:33.551968Z","iopub.execute_input":"2022-04-24T01:14:33.552189Z","iopub.status.idle":"2022-04-24T01:14:34.129019Z","shell.execute_reply.started":"2022-04-24T01:14:33.552163Z","shell.execute_reply":"2022-04-24T01:14:34.127493Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:34.130886Z","iopub.execute_input":"2022-04-24T01:14:34.131427Z","iopub.status.idle":"2022-04-24T01:14:34.260615Z","shell.execute_reply.started":"2022-04-24T01:14:34.131377Z","shell.execute_reply":"2022-04-24T01:14:34.259629Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:14:34.262670Z","iopub.execute_input":"2022-04-24T01:14:34.263023Z","iopub.status.idle":"2022-04-24T01:15:02.070591Z","shell.execute_reply.started":"2022-04-24T01:14:34.262977Z","shell.execute_reply":"2022-04-24T01:15:02.068969Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Topic Modeling using only lda_model yeilds interesting results. Although the vaccine is a redundant term coming out, and given the frequency of word 'vaccine' was the largest among all the comments, the topics are well divided. Even some topics can be distinguished by reading the the words. But since there are some redundant topics, we can reduce the number of topics and see if the topics are well divided without redundant topics.","metadata":{}},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:15:02.075013Z","iopub.execute_input":"2022-04-24T01:15:02.075311Z","iopub.status.idle":"2022-04-24T01:15:32.148119Z","shell.execute_reply.started":"2022-04-24T01:15:02.075277Z","shell.execute_reply":"2022-04-24T01:15:32.146254Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"At least the lda model with 'num_topics = 6' is less redundant, but now the words in each topics are less clear than lda model with 'num_topics = 10'. Let's see if the lda model with tf-idf model applied and see if there's any difference between simple lda model with lda model with tf-idf model.","metadata":{}},{"cell_type":"code","source":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print(f'Topic: {idx} Word: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:15:32.150026Z","iopub.execute_input":"2022-04-24T01:15:32.150392Z","iopub.status.idle":"2022-04-24T01:15:51.698353Z","shell.execute_reply.started":"2022-04-24T01:15:32.150342Z","shell.execute_reply":"2022-04-24T01:15:51.697522Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=6, id2word=dictionary, passes=2, workers=4)\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print(f'Topic: {idx} Word: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:15:51.700313Z","iopub.execute_input":"2022-04-24T01:15:51.700705Z","iopub.status.idle":"2022-04-24T01:16:11.166780Z","shell.execute_reply.started":"2022-04-24T01:15:51.700624Z","shell.execute_reply":"2022-04-24T01:16:11.165555Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"In case of lda model with tf-idf model applied, the weight of the vaccine has been dramatically reduced. However, some of the unnecessary words such as 'sure', 'reddit', yeah', which contain less information but oftenly used are included. There are some strength and weakness on the lda model with tf-idf model applied, but we decide to use simple lda model for further study since the words in the simple lda model contain more information.","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.2 Topic Modeling in Adjective","metadata":{}},{"cell_type":"code","source":"def lemmatize_stemming_adj(text):\n    snow_stemmer = snowball.SnowballStemmer(language='english')\n    return snow_stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='a'))\ndef preprocess_adj(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming_adj(token))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:16:11.168505Z","iopub.execute_input":"2022-04-24T01:16:11.168795Z","iopub.status.idle":"2022-04-24T01:16:11.176820Z","shell.execute_reply.started":"2022-04-24T01:16:11.168758Z","shell.execute_reply":"2022-04-24T01:16:11.175698Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"adj_processed_docs = df['comment_body'].map(preprocess_adj)\nadj_processed_docs[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:16:11.178720Z","iopub.execute_input":"2022-04-24T01:16:11.179176Z","iopub.status.idle":"2022-04-24T01:16:29.972643Z","shell.execute_reply.started":"2022-04-24T01:16:11.179131Z","shell.execute_reply":"2022-04-24T01:16:29.971809Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(adj_processed_docs)\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:16:29.973984Z","iopub.execute_input":"2022-04-24T01:16:29.974210Z","iopub.status.idle":"2022-04-24T01:16:31.177390Z","shell.execute_reply.started":"2022-04-24T01:16:29.974184Z","shell.execute_reply":"2022-04-24T01:16:31.175024Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=5000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in adj_processed_docs]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:16:31.179126Z","iopub.execute_input":"2022-04-24T01:16:31.179448Z","iopub.status.idle":"2022-04-24T01:16:31.733600Z","shell.execute_reply.started":"2022-04-24T01:16:31.179406Z","shell.execute_reply":"2022-04-24T01:16:31.732617Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:16:31.735120Z","iopub.execute_input":"2022-04-24T01:16:31.735387Z","iopub.status.idle":"2022-04-24T01:17:00.553701Z","shell.execute_reply.started":"2022-04-24T01:16:31.735356Z","shell.execute_reply":"2022-04-24T01:17:00.552699Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:17:00.555546Z","iopub.execute_input":"2022-04-24T01:17:00.555883Z","iopub.status.idle":"2022-04-24T01:17:29.830417Z","shell.execute_reply.started":"2022-04-24T01:17:00.555846Z","shell.execute_reply":"2022-04-24T01:17:29.829541Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Running lda model with only adjective led to bad performing result. The potential reasons behind is 1) the lemmatization wasn't perfectly done, we still can observe the verb although we have filtered adjective variable, 2) the adjective were used less frequently that some dominant words (i.e. vaccine, dose, trial, etc.) so few adjective words are being seen. Let's see if the same thing applies to noun and advervbs as well.","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.3 Topic Modeling in Noun","metadata":{}},{"cell_type":"code","source":"def lemmatize_stemming_noun(text):\n    snow_stemmer = snowball.SnowballStemmer(language='english')\n    return snow_stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='n'))\ndef preprocess_noun(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming_noun(token))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:17:29.831789Z","iopub.execute_input":"2022-04-24T01:17:29.832195Z","iopub.status.idle":"2022-04-24T01:17:29.839381Z","shell.execute_reply.started":"2022-04-24T01:17:29.832156Z","shell.execute_reply":"2022-04-24T01:17:29.838735Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"noun_processed_docs = df['comment_body'].map(preprocess_noun)\nnoun_processed_docs[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:17:29.840577Z","iopub.execute_input":"2022-04-24T01:17:29.841198Z","iopub.status.idle":"2022-04-24T01:17:49.029814Z","shell.execute_reply.started":"2022-04-24T01:17:29.841163Z","shell.execute_reply":"2022-04-24T01:17:49.028879Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(noun_processed_docs)\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:17:49.031344Z","iopub.execute_input":"2022-04-24T01:17:49.031843Z","iopub.status.idle":"2022-04-24T01:17:50.267487Z","shell.execute_reply.started":"2022-04-24T01:17:49.031797Z","shell.execute_reply":"2022-04-24T01:17:50.266623Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=5000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in noun_processed_docs]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:17:50.268935Z","iopub.execute_input":"2022-04-24T01:17:50.269173Z","iopub.status.idle":"2022-04-24T01:17:50.853700Z","shell.execute_reply.started":"2022-04-24T01:17:50.269144Z","shell.execute_reply":"2022-04-24T01:17:50.853054Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:17:50.854744Z","iopub.execute_input":"2022-04-24T01:17:50.855296Z","iopub.status.idle":"2022-04-24T01:18:17.963133Z","shell.execute_reply.started":"2022-04-24T01:17:50.855264Z","shell.execute_reply":"2022-04-24T01:18:17.961794Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:18:17.965013Z","iopub.execute_input":"2022-04-24T01:18:17.965283Z","iopub.status.idle":"2022-04-24T01:18:46.890230Z","shell.execute_reply.started":"2022-04-24T01:18:17.965251Z","shell.execute_reply":"2022-04-24T01:18:46.889276Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Just using noun tag gives less information about the topic. Since most of the words are noun, it feels like random sequences of words. We'll see if adverbs gives meaningful result.","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.4 Topic Modeling in Adverb","metadata":{}},{"cell_type":"code","source":"def lemmatize_stemming_adv(text):\n    snow_stemmer = snowball.SnowballStemmer(language='english')\n    return snow_stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='r'))\ndef preprocess_adv(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming_adv(token))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:18:46.891940Z","iopub.execute_input":"2022-04-24T01:18:46.892185Z","iopub.status.idle":"2022-04-24T01:18:46.899316Z","shell.execute_reply.started":"2022-04-24T01:18:46.892154Z","shell.execute_reply":"2022-04-24T01:18:46.898433Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"adv_processed_docs = df['comment_body'].map(preprocess_adv)\nadv_processed_docs[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:18:46.900902Z","iopub.execute_input":"2022-04-24T01:18:46.901550Z","iopub.status.idle":"2022-04-24T01:19:04.834565Z","shell.execute_reply.started":"2022-04-24T01:18:46.901505Z","shell.execute_reply":"2022-04-24T01:19:04.833670Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(adv_processed_docs)\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:19:04.835996Z","iopub.execute_input":"2022-04-24T01:19:04.836298Z","iopub.status.idle":"2022-04-24T01:19:06.035670Z","shell.execute_reply.started":"2022-04-24T01:19:04.836258Z","shell.execute_reply":"2022-04-24T01:19:06.034793Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=5000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in adv_processed_docs]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:19:06.041249Z","iopub.execute_input":"2022-04-24T01:19:06.041800Z","iopub.status.idle":"2022-04-24T01:19:06.993759Z","shell.execute_reply.started":"2022-04-24T01:19:06.041629Z","shell.execute_reply":"2022-04-24T01:19:06.992811Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:19:06.994921Z","iopub.execute_input":"2022-04-24T01:19:06.995143Z","iopub.status.idle":"2022-04-24T01:19:33.846537Z","shell.execute_reply.started":"2022-04-24T01:19:06.995117Z","shell.execute_reply":"2022-04-24T01:19:33.845581Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:19:33.848305Z","iopub.execute_input":"2022-04-24T01:19:33.848564Z","iopub.status.idle":"2022-04-24T01:20:03.191736Z","shell.execute_reply.started":"2022-04-24T01:19:33.848531Z","shell.execute_reply":"2022-04-24T01:20:03.190945Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"For the adverb part, similar to adjective, simple lda model with only adverb led to bad performing result. The lemmatizer hasn't identified adverb precisely. Maybe combination of pos tag might give us a better result, we run lda model with noun, verb, and adjective for the last time. and before running the model, we'll eliminate some words which appears often but not really necessary such as \"reddit\", \"https\", \"comment\", \"wiki\" and foul lauguage and informal words such as \"fuck\",\"boof\", and \"legit\".","metadata":{}},{"cell_type":"code","source":"def lemmatize_verb_adj_noun(sentence):\n    wnl = WordNetLemmatizer()\n    snow_stemmer = snowball.SnowballStemmer(language='english')\n    for word, tag in pos_tag(word_tokenize(sentence)):\n        if tag.startswith(\"NN\"):\n            return wnl.lemmatize(word, pos='n')\n        elif tag.startswith('VB'):\n            return snow_stemmer.stem(wnl.lemmatize(word, pos='v'))\n        elif tag.startswith('JJ'):\n            return snow_stemmer.stem(wnl.lemmatize(word, pos='a'))\n        else:\n            return word\n\ndef preprocess_verb_adj_noun(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_verb_adj_noun(token))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:20:03.193322Z","iopub.execute_input":"2022-04-24T01:20:03.193738Z","iopub.status.idle":"2022-04-24T01:20:03.202578Z","shell.execute_reply.started":"2022-04-24T01:20:03.193683Z","shell.execute_reply":"2022-04-24T01:20:03.201901Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"verb_adj_noun_processed_docs = df['comment_body'].map(preprocess_verb_adj_noun)\nverb_adj_noun_processed_docs[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:20:03.203666Z","iopub.execute_input":"2022-04-24T01:20:03.204441Z","iopub.status.idle":"2022-04-24T01:22:48.127076Z","shell.execute_reply.started":"2022-04-24T01:20:03.204407Z","shell.execute_reply":"2022-04-24T01:22:48.126252Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# omit unnecessary variables\nverb_adj_noun_processed_docs = [[element for element in sentence if element!=\"reddit\" and element!='subreddit'\n                                 and element!='vaccin' and element!= 'huuuuuuuge' and element!=\"https\" and element!=\"http\" \n                                 and element!=\"comment\" and element!=\"wiki\" and element!=\"fuck\" and element!=\"boof\" \n                                 and element!=\"legit\" and element!='post' and element!='question' and element!='discussion'\n                                 and element!=\"wiki_rule_\" and element!=\"delet\" and element!= \"remov\" and element!= \"like\"] for sentence in verb_adj_noun_processed_docs]\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:22:48.128433Z","iopub.execute_input":"2022-04-24T01:22:48.128716Z","iopub.status.idle":"2022-04-24T01:22:48.438699Z","shell.execute_reply.started":"2022-04-24T01:22:48.128646Z","shell.execute_reply":"2022-04-24T01:22:48.437857Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(verb_adj_noun_processed_docs)\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:22:48.440146Z","iopub.execute_input":"2022-04-24T01:22:48.440471Z","iopub.status.idle":"2022-04-24T01:22:49.591905Z","shell.execute_reply.started":"2022-04-24T01:22:48.440427Z","shell.execute_reply":"2022-04-24T01:22:49.590097Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=5000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in verb_adj_noun_processed_docs]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:22:49.593218Z","iopub.execute_input":"2022-04-24T01:22:49.593445Z","iopub.status.idle":"2022-04-24T01:22:50.141367Z","shell.execute_reply.started":"2022-04-24T01:22:49.593420Z","shell.execute_reply":"2022-04-24T01:22:50.140619Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=4, id2word=dictionary, passes=2, workers=2)\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic: {idx} \\nWord)s: {topic}')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:22:50.142528Z","iopub.execute_input":"2022-04-24T01:22:50.142796Z","iopub.status.idle":"2022-04-24T01:23:20.360720Z","shell.execute_reply.started":"2022-04-24T01:22:50.142766Z","shell.execute_reply":"2022-04-24T01:23:20.359499Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Some of the topics are hard to identify since a lot of vocabularies are redundant among the topics, but still we can distinguish the topics by the other keywords. Since we have eliminated few words that are not significant or detrimental, we assume this model yeilds the best result. For the number of topic, since the topics are pretty limited (It's about covid and vaccination, we assume the topic is not as wide as lda of random test) so we decide to reduce the num_topics to 4.","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"STOP_WORDS = nltk.corpus.stopwords.words()\n\ndef clean_sentence(val):\n    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n    regex = re.compile('([^\\s\\w?!]|_)')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)\n            \n    sentence = \" \".join(sentence)\n    return sentence\n\ndef clean_dataframe(data):\n    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n    data = data.dropna(how=\"any\")\n    \n    data['comment_body'] = data['comment_body'].apply(clean_sentence)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:23:20.362762Z","iopub.execute_input":"2022-04-24T01:23:20.363021Z","iopub.status.idle":"2022-04-24T01:23:20.394871Z","shell.execute_reply.started":"2022-04-24T01:23:20.362989Z","shell.execute_reply":"2022-04-24T01:23:20.393845Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"analyser = SentimentIntensityAnalyzer()\n\ndef sentiment_analyzer_scores(sentence):\n    score = analyser.polarity_scores(sentence)\n    print(f\"{sentence} {str(score)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:23:20.396132Z","iopub.execute_input":"2022-04-24T01:23:20.396364Z","iopub.status.idle":"2022-04-24T01:23:20.417001Z","shell.execute_reply.started":"2022-04-24T01:23:20.396337Z","shell.execute_reply":"2022-04-24T01:23:20.416081Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df = clean_dataframe(df)\ndf['comment_body']","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:23:20.418316Z","iopub.execute_input":"2022-04-24T01:23:20.418823Z","iopub.status.idle":"2022-04-24T01:24:34.965742Z","shell.execute_reply.started":"2022-04-24T01:23:20.418778Z","shell.execute_reply":"2022-04-24T01:24:34.964774Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"analyser.polarity_scores(df['comment_body'][3])['compound']","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:34.967364Z","iopub.execute_input":"2022-04-24T01:24:34.967576Z","iopub.status.idle":"2022-04-24T01:24:34.976947Z","shell.execute_reply.started":"2022-04-24T01:24:34.967551Z","shell.execute_reply":"2022-04-24T01:24:34.976112Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"sentiment = []\nfor i in range(len(df['comment_body'])):\n    sentiment.append(analyser.polarity_scores(df['comment_body'][i])['compound'])\nsentiment = pd.Series(sentiment)\ndf = pd.concat([df,sentiment], axis=1)\ndf.columns = ['post_id','post_author','post_date','post_title','post_score','post_permalink','post_url','comment_id',\n              'comment_author','comment_date','comment_parent_id','comment_edited','comment_score','comment_body','sentiment']","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:34.978236Z","iopub.execute_input":"2022-04-24T01:24:34.978479Z","iopub.status.idle":"2022-04-24T01:24:43.607699Z","shell.execute_reply.started":"2022-04-24T01:24:34.978451Z","shell.execute_reply":"2022-04-24T01:24:43.606925Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:43.609107Z","iopub.execute_input":"2022-04-24T01:24:43.609578Z","iopub.status.idle":"2022-04-24T01:24:43.630468Z","shell.execute_reply.started":"2022-04-24T01:24:43.609546Z","shell.execute_reply":"2022-04-24T01:24:43.629811Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].plot.kde(ind=[-1, -0.5, 0, 0.5, 1],figsize=(15,15))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:43.631851Z","iopub.execute_input":"2022-04-24T01:24:43.632297Z","iopub.status.idle":"2022-04-24T01:24:44.016907Z","shell.execute_reply.started":"2022-04-24T01:24:43.632261Z","shell.execute_reply":"2022-04-24T01:24:44.016121Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"The reason why there are huge spike in 0.00 is because there are a lot of variable with the compound sentiment with 0.00 value, which means the segmentation of positive, neutral, negative tone came out as 100% neutral (with neu: 1.0 value). Since the comments are short and some of them contains only factual informations, having substantial amount of compound score of 0.0 is plausible.","metadata":{}},{"cell_type":"code","source":"pos = 0\nneu = 0\nneg = 0\nfor i in range(len(df['sentiment'])):\n    if df['sentiment'][i] == 0.0:\n        neu += 1\n    elif df['sentiment'][i] > 0.0:\n        pos += 1\n    else:\n        neg += 1\n        \nprint(f\"positive: {pos}, neutral: {neu}, negative: {neg}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:44.018362Z","iopub.execute_input":"2022-04-24T01:24:44.018745Z","iopub.status.idle":"2022-04-24T01:24:44.587444Z","shell.execute_reply.started":"2022-04-24T01:24:44.018706Z","shell.execute_reply":"2022-04-24T01:24:44.586307Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"data = {'positive': pos, 'neutral': neu, 'negative': neg}\nsentiment_series = pd.Series(data, index = ['positive', 'neutral', 'negative'])\nsentiment_series.plot.bar(rot = 0, figsize=(15,15))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:44.588628Z","iopub.execute_input":"2022-04-24T01:24:44.588901Z","iopub.status.idle":"2022-04-24T01:24:44.849261Z","shell.execute_reply.started":"2022-04-24T01:24:44.588871Z","shell.execute_reply":"2022-04-24T01:24:44.848432Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Looking at the barplot, we can see that although there is the highest spike on the 0.0, the total number of neutral comments are the least. One of the interesting findings is that the number of positive comments are much more than the neutral and negative comments. The possible reason for the phenomena is because that the dictionary tends to evaluate the comment positive.","metadata":{}},{"cell_type":"markdown","source":"## Adding covariates to topic modeling","metadata":{}},{"cell_type":"code","source":"lda_model.print_topics(-1,7)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:44.850602Z","iopub.execute_input":"2022-04-24T01:24:44.850887Z","iopub.status.idle":"2022-04-24T01:24:44.858398Z","shell.execute_reply.started":"2022-04-24T01:24:44.850859Z","shell.execute_reply":"2022-04-24T01:24:44.857492Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"following is the available topics for the finalized lda_model, we now need to extract the dominant topic of each sentence, fit it with the sentiment model to check how topics vary by the sentiment. We then need to check how to find the dominant topic in each sentence.","metadata":{}},{"cell_type":"code","source":"for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 20)))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:44.859743Z","iopub.execute_input":"2022-04-24T01:24:44.860099Z","iopub.status.idle":"2022-04-24T01:24:44.877594Z","shell.execute_reply.started":"2022-04-24T01:24:44.860047Z","shell.execute_reply":"2022-04-24T01:24:44.876430Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Test version to check the score of each topic for the sentence index 1. We can measure score by \n'lda_model[bow_corpus][sentence index]'.","metadata":{}},{"cell_type":"code","source":"lda_model[bow_corpus][4]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:44.879259Z","iopub.execute_input":"2022-04-24T01:24:44.879874Z","iopub.status.idle":"2022-04-24T01:24:44.892773Z","shell.execute_reply.started":"2022-04-24T01:24:44.879827Z","shell.execute_reply":"2022-04-24T01:24:44.891831Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"dominant_topic = []\ndominant_topic_score = []\nfor i in range(len(bow_corpus)):\n    array = np.array(lda_model[bow_corpus][i])[:,1]\n    dominant_topic.append(array.argmax())\n    dominant_topic_score.append(array[array.argmax()])\nprint(dominant_topic[:10])\nprint(dominant_topic_score[:10])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:24:44.893944Z","iopub.execute_input":"2022-04-24T01:24:44.894169Z","iopub.status.idle":"2022-04-24T01:25:03.484133Z","shell.execute_reply.started":"2022-04-24T01:24:44.894140Z","shell.execute_reply":"2022-04-24T01:25:03.483014Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"dominant_topic_df = pd.DataFrame({'dominant_topic':dominant_topic, 'dominant_topic_score':dominant_topic_score})\ndf = pd.concat([df,dominant_topic_df], axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:25:03.485689Z","iopub.execute_input":"2022-04-24T01:25:03.486013Z","iopub.status.idle":"2022-04-24T01:25:03.545649Z","shell.execute_reply.started":"2022-04-24T01:25:03.485971Z","shell.execute_reply":"2022-04-24T01:25:03.545004Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Now we divide up the df into 3 dfs: df with positive sentiment, df with neutral sentiment, and df with negative sentiment.","metadata":{}},{"cell_type":"code","source":"df_positive = df.query('sentiment > 0.0')\ndf_neutral = df.query('sentiment == 0.0')\ndf_negative = df.query('sentiment < 0.0')\n\nprint(df_positive.shape[0])\nprint(df_neutral.shape[0])\nprint(df_negative.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:25:03.547033Z","iopub.execute_input":"2022-04-24T01:25:03.547529Z","iopub.status.idle":"2022-04-24T01:25:03.593030Z","shell.execute_reply.started":"2022-04-24T01:25:03.547498Z","shell.execute_reply":"2022-04-24T01:25:03.592258Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"pos_grouped = df_positive.groupby('dominant_topic').size()\nneu_grouped = df_neutral.groupby('dominant_topic').size()\nneg_grouped = df_negative.groupby('dominant_topic').size()\n\ngrouped_size = pd.concat([neg_grouped,neu_grouped,pos_grouped], axis = 1)\ngrouped_size.columns = ['neg_dominant', 'neu_dominant', 'pos_dominant']\ngrouped_size.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:25:03.594475Z","iopub.execute_input":"2022-04-24T01:25:03.594761Z","iopub.status.idle":"2022-04-24T01:25:03.612243Z","shell.execute_reply.started":"2022-04-24T01:25:03.594722Z","shell.execute_reply":"2022-04-24T01:25:03.611599Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"since the # of observations of the each df is different, we try to normalize the data by dividing it with the length of each columns.","metadata":{}},{"cell_type":"code","source":"grouped_size['pos_dominant'] = grouped_size['pos_dominant'].div(df_positive.shape[0])\ngrouped_size['neu_dominant'] = grouped_size['neu_dominant'].div(df_neutral.shape[0])\ngrouped_size['neg_dominant'] = grouped_size['neg_dominant'].div(df_negative.shape[0])\ngrouped_size.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:25:03.613178Z","iopub.execute_input":"2022-04-24T01:25:03.613667Z","iopub.status.idle":"2022-04-24T01:25:03.627920Z","shell.execute_reply.started":"2022-04-24T01:25:03.613624Z","shell.execute_reply":"2022-04-24T01:25:03.627298Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Now we can see some pattens in the dataframe. we now visualize the result.","metadata":{}},{"cell_type":"code","source":"line_plot = grouped_size.T.plot.line(figsize=(15,15))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:25:03.628874Z","iopub.execute_input":"2022-04-24T01:25:03.629413Z","iopub.status.idle":"2022-04-24T01:25:03.975828Z","shell.execute_reply.started":"2022-04-24T01:25:03.629384Z","shell.execute_reply":"2022-04-24T01:25:03.974983Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"However looking at the line plot with 3 bin is not sufficient, so we do the same thing with more bin with binwidth of 0.2. But there are some interesting trends in the graph such as topic 0. We'll examine more when the binwidth is narrower.","metadata":{}},{"cell_type":"code","source":"df_most_positive = df.query('sentiment > 0.8')\ndf_quite_positive = df.query('sentiment <= 0.8 & sentiment > 0.6')\ndf_somewhat_positive = df.query('sentiment <= 0.6 & sentiment > 0.4')\ndf_abit_positive = df.query('sentiment <= 0.4 & sentiment > 0.2')\ndf_merely_positive = df.query('sentiment <= 0.2 & sentiment > 0.0')\ndf_neutral = df.query('sentiment == 0.0')\ndf_merely_negative = df.query('sentiment < 0.0 & sentiment >= -0.2')\ndf_abit_negative = df.query('sentiment < -0.2 & sentiment >= -0.4')\ndf_somewhat_negative = df.query('sentiment < -0.4 & sentiment >= -0.6')\ndf_quite_negative = df.query('sentiment < -0.6 & sentiment >= -0.8')\ndf_most_negative = df.query('sentiment < -0.8')\n\nmost_positive_grouped = df_most_positive.groupby('dominant_topic').size()\nquite_positive_grouped = df_quite_positive.groupby('dominant_topic').size()\nsomewhat_positive_grouped = df_somewhat_positive.groupby('dominant_topic').size()\nabit_positive_grouped = df_abit_positive.groupby('dominant_topic').size()\nmerely_positive_grouped = df_merely_positive.groupby('dominant_topic').size()\nneutral_grouped = df_neutral.groupby('dominant_topic').size()\nmerely_negative_grouped = df_merely_negative.groupby('dominant_topic').size()\nabit_negative_grouped = df_abit_negative.groupby('dominant_topic').size()\nsomewhat_negative_grouped = df_somewhat_negative.groupby('dominant_topic').size()\nquite_negative_grouped = df_quite_negative.groupby('dominant_topic').size()\nmost_negative_grouped = df_most_negative.groupby('dominant_topic').size()\n\ngrouped_size = pd.concat([most_negative_grouped,quite_negative_grouped,somewhat_negative_grouped,\n                         abit_negative_grouped, merely_negative_grouped, neutral_grouped,\n                         merely_positive_grouped, abit_positive_grouped, somewhat_positive_grouped,\n                         quite_positive_grouped, most_positive_grouped], axis = 1)\ngrouped_size.columns = ['most_neg_dom', 'quite_neg_dom', 'somewhat_neg_dom',\n                       'abit_neg_dom', 'merely_neg_dom', 'neu_dom',\n                       'merely_pos_dom', 'abit_pos_dom', 'somewhat_pos_dom',\n                       'quite_pos_dom', 'most_pos_dom']\ngrouped_size['most_neg_dom'] = grouped_size['most_neg_dom'].div(df_most_negative.shape[0])\ngrouped_size['quite_neg_dom'] = grouped_size['quite_neg_dom'].div(df_quite_negative.shape[0])\ngrouped_size['somewhat_neg_dom'] = grouped_size['somewhat_neg_dom'].div(df_somewhat_negative.shape[0])\ngrouped_size['abit_neg_dom'] = grouped_size['abit_neg_dom'].div(df_abit_negative.shape[0])\ngrouped_size['merely_neg_dom'] = grouped_size['merely_neg_dom'].div(df_merely_negative.shape[0])\ngrouped_size['neu_dom'] = grouped_size['neu_dom'].div(df_neutral.shape[0])\ngrouped_size['merely_pos_dom'] = grouped_size['merely_pos_dom'].div(df_merely_positive.shape[0])\ngrouped_size['abit_pos_dom'] = grouped_size['abit_pos_dom'].div(df_abit_positive.shape[0])\ngrouped_size['somewhat_pos_dom'] = grouped_size['somewhat_pos_dom'].div(df_somewhat_positive.shape[0])\ngrouped_size['quite_pos_dom'] = grouped_size['quite_pos_dom'].div(df_quite_positive.shape[0])\ngrouped_size['most_pos_dom'] = grouped_size['most_pos_dom'].div(df_most_positive.shape[0])\n\ngrouped_size.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:25:03.977221Z","iopub.execute_input":"2022-04-24T01:25:03.977479Z","iopub.status.idle":"2022-04-24T01:25:04.083700Z","shell.execute_reply.started":"2022-04-24T01:25:03.977451Z","shell.execute_reply":"2022-04-24T01:25:04.082952Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"line_plot = grouped_size.T.plot.line(figsize=(16,8))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:25:04.085198Z","iopub.execute_input":"2022-04-24T01:25:04.085870Z","iopub.status.idle":"2022-04-24T01:25:04.392482Z","shell.execute_reply.started":"2022-04-24T01:25:04.085827Z","shell.execute_reply":"2022-04-24T01:25:04.391531Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"The trends are more angular, but the overall trends doesn't differ too much from the previous graph with binwidth 3. The interesting findings are, people who tends to leave comments related to topic 0 has neutral and positive feelings, while comments with topic 1 tend to contain some kind of emotions, topic 1 slightly tends to be negative. Topic 2 and 3 shows no trend.\n\nJust to Quantify the data, let's see the regression details for each topics and see the tendency of being positive, neutral, or negative based on the topic.","metadata":{}},{"cell_type":"code","source":"df_dom_top_0 = df.query('dominant_topic == 0')\ndf_dom_top_1 = df.query('dominant_topic == 1')\ndf_dom_top_2 = df.query('dominant_topic == 2')\ndf_dom_top_3 = df.query('dominant_topic == 3')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:32:51.784214Z","iopub.execute_input":"2022-04-24T01:32:51.784518Z","iopub.status.idle":"2022-04-24T01:32:51.823165Z","shell.execute_reply.started":"2022-04-24T01:32:51.784488Z","shell.execute_reply":"2022-04-24T01:32:51.822219Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"top0_results = smf.ols('dominant_topic_score ~ sentiment', data=df_dom_top_0).fit()\nprint(top0_results.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:34:53.830293Z","iopub.execute_input":"2022-04-24T01:34:53.830591Z","iopub.status.idle":"2022-04-24T01:34:53.869455Z","shell.execute_reply.started":"2022-04-24T01:34:53.830558Z","shell.execute_reply":"2022-04-24T01:34:53.868207Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"top1_results = smf.ols('dominant_topic_score ~ sentiment', data=df_dom_top_1).fit()\nprint(top1_results.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:36:01.668444Z","iopub.execute_input":"2022-04-24T01:36:01.669602Z","iopub.status.idle":"2022-04-24T01:36:01.701166Z","shell.execute_reply.started":"2022-04-24T01:36:01.669546Z","shell.execute_reply":"2022-04-24T01:36:01.700213Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"top2_results = smf.ols('dominant_topic_score ~ sentiment', data=df_dom_top_2).fit()\nprint(top2_results.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:36:06.207819Z","iopub.execute_input":"2022-04-24T01:36:06.208091Z","iopub.status.idle":"2022-04-24T01:36:06.236089Z","shell.execute_reply.started":"2022-04-24T01:36:06.208063Z","shell.execute_reply":"2022-04-24T01:36:06.235152Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"top3_results = smf.ols('dominant_topic_score ~ sentiment', data=df_dom_top_3).fit()\nprint(top3_results.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T01:36:10.011072Z","iopub.execute_input":"2022-04-24T01:36:10.011407Z","iopub.status.idle":"2022-04-24T01:36:10.033525Z","shell.execute_reply.started":"2022-04-24T01:36:10.011371Z","shell.execute_reply":"2022-04-24T01:36:10.032557Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"The result differ with r because of the way of 1)methodology of evaluating the sentiment was different between python and R (We used vaderSentiment analyzer in python, the sentiment analysis model for unsupervised model which returns positive, negative, neutral sentiment for each sentences, while for r, we have imported dictionary of positive and negative words, count the appearence in sentence and divided them by number of token in the sentence) 2) methodology evaluating covarates were different (for python, we have created 2 new columns, dominant topic and its intensity, dived the dataframe by sentence topic and created linear regression model with the intensity of the topic with the sentiment, while we used stm package in r). Since the methodology was evaluating sentiment and methodology fitting sentiment as a covariate were different, these two models gave entirely different results.","metadata":{}}]}
